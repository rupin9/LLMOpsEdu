{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mCKOk8jj2jmB"
   },
   "source": [
    "### OpenAI API를 ChatGPT 활용실습\n",
    "\n",
    "이 노트북에서는 OpenAI API를 사용하여 ChatGPT와 상호작용하는 방법을 배웁니다.\n",
    "우리는 간단한 예제를 통해 API를 호출하고 응답을 받아오는 과정을 살펴볼 것입니다.\n",
    "\n",
    "## 필요한 라이브러리 설치\n",
    "\n",
    "OpenAI 라이브러리가 설치되어 있지 않다면, 아래 명령어를 실행하여 설치하세요\n",
    "\n",
    "!pip install openai\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai in ./.local/lib/python3.10/site-packages (1.37.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./.local/lib/python3.10/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./.local/lib/python3.10/site-packages (from openai) (2.8.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in ./.local/lib/python3.10/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./.local/lib/python3.10/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./.local/lib/python3.10/site-packages (from openai) (4.4.0)\n",
      "Requirement already satisfied: tqdm>4 in ./.local/lib/python3.10/site-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: sniffio in ./.local/lib/python3.10/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in ./.local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in ./.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: certifi in ./.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in ./.local/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./.local/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7mSlV-AxiqkQ",
    "outputId": "1e797fec-973f-4275-876a-5f82f8017bc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "물론입니다! 파이썬으로 \"Hello, World!\"를 출력하는 코드는 다음과 같습니다.\n",
      "\n",
      "```python\n",
      "print(\"Hello, World!\")\n",
      "```\n",
      "\n",
      "이 코드를 실행하면 콘솔에 `Hello, World!`라는 문구가 출력됩니다.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# OpenAI API 클라이언트 생성\n",
    "client = OpenAI(api_key='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')\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"당신은 코드 작성에 도움이 되는 프로그래머입니다.\"},\n",
    "        {\"role\": \"user\", \"content\": \"파이썬으로 Hello World 출력하는 코드를 작성해줘.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sq-S1L8ObyJK"
   },
   "source": [
    "\n",
    "위 코드를 실행하면 ChatGPT가 \"Hello World\"를 출력하는 파이썬 코드를 생성합니다.\n",
    "ChatGPT는 요청에 따라 간단하고 정확한 파이썬 코드를 제공했습니다.\n",
    "이 응답은 ChatGPT가 프로그래밍 관련 질문에 대해 명확하고 실용적인 답변을 제공할 수 있음을 보여줍니다.\n",
    "\n",
    "이러한 방식으로 OpenAI API를 사용하여 다양한 프로그래밍 관련 질문이나 작업에 대해 도움을 받을 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RQoVw9YIc2WX"
   },
   "source": [
    "### OpenAI API를 활용한 코드 생성: 프롬프트와 옵션의 영향 - temperature , top_p\n",
    "\n",
    "\n",
    "이 노트북에서는 OpenAI API를 사용하여 코드를 생성할 때, 프롬프트와 옵션 설정이 결과에 미치는 영향을 살펴봅니다.\n",
    "\n",
    "## 1. 기본 프롬프트 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "niJqK50Pre_x",
    "outputId": "710c8c94-a631-4745-e174-14c7348c2a23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quick sort 프로그램을 작성해줘\n",
      "봇: 퀵 정렬(Quick Sort)은 효율적인 정렬 알고리즘 중 하나입니다. 파이썬으로 간단한 퀵 정렬 프로그램을 작성해 보겠습니다.\n",
      "\n",
      "```python\n",
      "def quick_sort(arr):\n",
      "    if len(arr) <= 1:\n",
      "        return arr\n",
      "    else:\n",
      "        pivot = arr[len(arr) // 2]  # 피벗을 배열의 중간 요소로 설정\n",
      "        left = [x for x in arr if x < pivot]  # 피벗보다 작은 요소들\n",
      "        middle = [x for x in arr if x == pivot]  # 피벗과 같은 요소들\n",
      "        right = [x for x in arr if x > pivot]  # 피벗보다 큰 요소들\n",
      "        return quick_sort(left) + middle + quick_sort(right)\n",
      "\n",
      "# 예제 사용\n",
      "if __name__ == \"__main__\":\n",
      "    data = [3, 6, 8, 10, 1, 2, 1]\n",
      "    sorted_data = quick_sort(data)\n",
      "    print(\"정렬된 배열:\", sorted_data)\n",
      "```\n",
      "\n",
      "위 코드는 배열을 입력으로 받아 퀵 정렬을 수행한 후, 정렬된 배열을 반환합니다. 실행하면 주어진 배열이 정렬되어 출력됩니다. 필요에 따라 배열의 요소 또는 피벗 선택 방법을 수정할 수도 있습니다.\n"
     ]
    }
   ],
   "source": [
    "def generate_code_response(user_input):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"당신은 코드 작성에 도움이 되는 프로그래머입니다.\"},\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# 사용자 입력 받기\n",
    "#파이썬으로 리스트의 모든 요소를 더하는 코드를 작성해줘.\n",
    "user_input = input()\n",
    "# 코드 작성 도움 챗봇 응답 생성\n",
    "bot_response = generate_code_response(user_input)\n",
    "print(f\"봇: {bot_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWMY_4vnc_64"
   },
   "source": [
    "## 2. 창의적 프롬프트 및 옵션 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TKyDRNlwj_7c",
    "outputId": "3cf4e0f0-0d8e-467c-c9cd-d4d5f62c1976"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "올해 여름에 유행하는 50대 남자의 패션을 추천해줘\n",
      "봇: 올해 여름에 유행하는 50대 남성 패션은 편안함과 스타일을 동시에 고려한 아이템들이 많이 등장하고 있습니다. 다음은 추천하는 스타일입니다:\n",
      "\n",
      "1. **린넨 셔츠**: 통기성이 좋고 시원한 느낌을 주는 린넨 소재의 셔츠. 밝은 컬러나 자연스러운 패턴이 여름에 잘 어울립니다. 소매를 살짝 롤업해 캐주얼한 느낌을 줄 수 있습니다.\n",
      "\n",
      "2. **슬랙스 또는 면 바지**: 편안하면서도 세련된 느낌을 줄 수 있는 슬랙스 또는 면 바지. 밝은 컬러 또는 파스텔톤의 바지가 여름에 잘 어울립니다.\n",
      "\n",
      "3. **반바지**: 편안하면서도 스타일리시한 반바지. 허리선이 높은 디자인의 반바지나 체크 패턴의 반바지가 트렌디합니다.\n",
      "\n",
      "4. **피케 셔츠**: 캐주얼하면서도 깔끔한 느낌을 주는 피케 셔트. 다양한 색상과 패턴으로 선택할 수 있으며, 바지와의 조화도 좋습니다.\n",
      "\n",
      "5. **스트라이프 티셔츠**: 여름철에 잘 어울리는 경쾌한 느낌의 스트라이프 티셔츠. 간단한 아이템이지만, 스타일을 살리는 데 큰 역할을 합니다.\n",
      "\n",
      "6. **가벼운 자켓**: 아침 저녁으로 쌀쌀할 수 있는 날씨를 대비하여 가벼운 자켓을 준비하세요. 린넨 소재나 얇은 면 자켓이 좋습니다.\n",
      "\n",
      "7. **편안한 신발**: 스니커즈, 로퍼, 샌들 중에서 선택할 수 있으며, 편안함과 스타일을 모두 잡을 수 있는 디자인을 추천합니다.\n",
      "\n",
      "8. **액세서리**: 부드러운 컬러의 모자나 선글라스, 간단한 팔찌 등으로 포인트를 주면 좋습니다.\n",
      "\n",
      "여름철에는 편안하고 쾌적한 스타일이 중요하니, 다양한 색상과 패턴을 활용해보세요!\n"
     ]
    }
   ],
   "source": [
    "def generate_code_response(user_input):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"당신은 코드 작성에 도움이 되는 프로그래머입니다.\"},\n",
    "            #{\"role\": \"system\", \"content\": \"당신은 패션디자이너 입니다.\"},\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "#\"파이썬으로 랜덤한 숫자를 생성하는 코드를 작성해줘. \"\n",
    "user_input = input()\n",
    "bot_response = generate_code_response(user_input)\n",
    "print(f\"봇: {bot_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_C3s1-VdDg6"
   },
   "source": [
    "## 창의적 프롬프트 및 옵션 설정 후\n",
    "\n",
    "1. 시스템 프롬프트 차이:\n",
    "   - 기본: \"당신은 코드 작성에 도움이 되는 프로그래머입니다.\"\n",
    "   - 창의적: \"당신은 창의적인 프로그래머입니다.\"\n",
    "   \n",
    "   이 차이는 AI의 응답 스타일에 영향을 줄 수 있습니다. \"창의적인 프로그래머\"라는 프롬프트는\n",
    "   AI에게 더 독특하거나 혁신적인 접근 방식을 취하도록 유도할 수 있습니다.\n",
    "\n",
    "2. 옵션 차이:\n",
    "   - 기본: 기본 설정 사용\n",
    "   - 창의적: temperature=0.85, top_p=0.95 설정\n",
    "   \n",
    "   temperature와 top_p 값을 높이면 AI의 응답이 더 다양하고 예측 불가능해질 수 있습니다.\n",
    "   이는 더 창의적이거나 독특한 코드 생성으로 이어질 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gtwtjrEtqvF5",
    "outputId": "73f0f4ae-d97c-46c1-cd86-dbcf86dfdfdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top_p의 값은 어떤 역할을 하는 거야?\n",
      "봇: `top_p`는 자연어 처리 모델, 특히 텍스트 생성 모델에서 사용되는 샘플링 기법인 '누적 확률 샘플링'과 관련이 있습니다. 이는 생성 과정에서 모델이 다음 단어를 선택할 때, 가장 높은 확률의 단어들만 고려하는 방법입니다.\n",
      "\n",
      "`top_p`는 다음과 같은 방식으로 작동합니다:\n",
      "\n",
      "1. 모델은 다음 단어에 대한 확률 분포를 생성합니다.\n",
      "2. 확률이 높은 단어들을 내림차순으로 정렬합니다.\n",
      "3. 이 단어들의 누적 확률을 계산하고, 누적 확률이 `top_p` 값에 도달할 때까지 단어들을 선택합니다.\n",
      "4. 최종적으로 선택된 단어 집합 중에서 무작위로 하나를 선택하여 다음 단어로 사용합니다.\n",
      "\n",
      "예를 들어, `top_p` 값이 0.9라면, 누적 확률이 90%에 해당하는 단어들만 고려하게 됩니다. 이로 인해 모델이 다양한 응답을 생성할 수 있지만, 너무 높은 `top_p` 값을 설정하면 일관성이 떨어질 수 있습니다.\n",
      "\n",
      "요약하자면, `top_p`는 텍스트 생성의 다양성과 품질을 조절하는 중요한 하이퍼파라미터입니다.\n"
     ]
    }
   ],
   "source": [
    "def generate_creative_code_response(user_input):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"당신은 창의적인 프로그래머입니다.\"},\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ],\n",
    "        temperature=0.95,\n",
    "        top_p=0.95\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "#파이썬으로 랜덤한 숫자를 생성하는 코드를 작성해줘.\n",
    "user_input = input()\n",
    "bot_response = generate_creative_code_response(user_input)\n",
    "print(f\"봇: {bot_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wa2RFUmvd1lk"
   },
   "source": [
    "결과 비교\n",
    "   - 기본 프롬프트 응답은 더 포괄적이고 교육적인 접근을 취했습니다. 여러 가지 랜덤 숫자 생성\n",
    "     방법을 소개하고 각각에 대한 설명을 포함했습니다.\n",
    "   - 창의적 프롬프트 응답은 더 간결하고 직접적인 접근을 취했습니다. 하나의 특정 예제에\n",
    "     집중하여 더 간단한 코드를 제시했습니다.\n",
    "\n",
    "이러한 차이는 프롬프트 설정과 옵션 조정이 AI의 응답 스타일과 내용에 큰 영향을 미칠 수 있음을\n",
    "보여줍니다. 사용자의 필요에 따라 이러한 설정을 조정하여 원하는 스타일의 코드나 설명을 얻을 수\n",
    "있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uocu7GdS-Dax"
   },
   "source": [
    "### OpenAI API를 활용한 코드 생성: 페널티 옵션의 영향\n",
    "\n",
    "이번에는 OpenAI API를 사용하여 코드를 생성할 때, 프리퀀시 페널티와 프레즌스 페널티 설정이 결과에 미치는 영향을 살펴봅니다.\n",
    "\n",
    "## 1. 기본 설정 (페널티 없음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "trIF_JFN8SLz",
    "outputId": "8dec38e2-0a52-4aa4-8f49-829308903f3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파이썬으로 파일을 읽고 내용을 출력하는 코드를 작성해줘.\n",
      "봇: 물론입니다! 아래는 파이썬으로 파일을 읽고 그 내용을 출력하는 간단한 코드 예제입니다.\n",
      "\n",
      "```python\n",
      "# 파일 경로를 지정합니다.\n",
      "file_path = 'example.txt'\n",
      "\n",
      "try:\n",
      "    # 파일을 읽기 모드로 엽니다.\n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        # 파일의 모든 내용을 읽어옵니다.\n",
      "        content = file.read()\n",
      "        # 읽어온 내용을 출력합니다.\n",
      "        print(content)\n",
      "except FileNotFoundError:\n",
      "    print(f\"{file_path} 파일이 존재하지 않습니다.\")\n",
      "except Exception as e:\n",
      "    print(f\"파일을 읽는 도중 오류가 발생했습니다: {e}\")\n",
      "```\n",
      "\n",
      "위 코드는 `example.txt`라는 파일을 읽어 그 내용을 출력합니다. 파일 경로를 원하는 파일로 변경하여 사용할 수 있습니다. 이 코드는 파일이 존재하지 않을 경우와 다른 오류가 발생할 경우에 대한 예외 처리도 포함되어 있습니다.\n"
     ]
    }
   ],
   "source": [
    "def generate_code_response(user_input):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"당신은 코드 작성에 도움이 되는 프로그래머입니다.\"},\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "#\"파이썬으로 파일을 읽고 내용을 출력하는 코드를 작성해줘. \"\n",
    "user_input = input()\n",
    "bot_response = generate_code_response(user_input)\n",
    "print(f\"봇: {bot_response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JqtSM4e2-OOa"
   },
   "source": [
    "## 2. 페널티 설정\n",
    "\n",
    "1. 프리퀀시 페널티 (frequency_penalty=0.9):\n",
    "   - 이 설정은 모델이 같은 단어나 구문을 반복하는 것을 줄입니다.\n",
    "   - 결과적으로, 페널티 설정 응답에서는 코드 설명이 더 다양한 표현을 사용하고 있습니다.(주석 등)\n",
    "\n",
    "2. 프레즌스 페널티 (presence_penalty=0.6):\n",
    "   - 이 설정은 모델이 새로운 주제나 아이디어를 도입하도록 장려합니다.\n",
    "   - 페널티 설정 응답에서는 파일 읽기의 기본 개념 외에도 추가적인 정보(예: 파일 경로 조정)를 제공하고 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vpbvAxdl-ExL",
    "outputId": "9303a7f7-3534-4cc7-a6cd-70d10cfef606"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "파이썬으로 파일을 읽고 내용을 출력하는 코드를 작성해줘.\n",
      "봇: 물론입니다! 아래는 파이썬으로 파일을 읽고 그 내용을 출력하는 간단한 코드 예제입니다.\n",
      "\n",
      "```python\n",
      "# 파일 경로를 지정합니다.\n",
      "file_path = 'example.txt'\n",
      "\n",
      "try:\n",
      "    # 파일을 읽기 모드로 엽니다.\n",
      "    with open(file_path, 'r', encoding='utf-8') as file:\n",
      "        # 파일의 각 줄을 읽어와서 출력합니다.\n",
      "        for line in file:\n",
      "            print(line.strip())  # strip()은 줄 끝의 개행 문자를 제거합니다.\n",
      "\n",
      "except FileNotFoundError:\n",
      "    print(f\"파일 '{file_path}' 을 찾을 수 없습니다.\")\n",
      "except Exception as e:\n",
      "    print(f\"오류 발생: {e}\")\n",
      "```\n",
      "\n",
      "위 코드는 `example.txt` 라는 이름의 텍스트 파일을 열고, 각 줄을 읽어서 화면에 출력합니다. 만약 파일이 존재하지 않거나 다른 오류가 발생하면 그에 대한 적절한 메시지를 출력합니다.\n",
      "\n",
      "파일 이름이나 경로를 필요에 맞게 수정하여 사용하시면 됩니다.\n"
     ]
    }
   ],
   "source": [
    "def generate_varied_code_response(user_input):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"당신은 다양한 방식으로 코드를 작성할 수 있는 프로그래머입니다.\"},\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ],\n",
    "        frequency_penalty=0.9,\n",
    "        presence_penalty=0.6\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "#파이썬으로 파일을 읽고 내용을 출력하는 코드를 작성해줘.\n",
    "user_input = input()\n",
    "bot_response = generate_varied_code_response(user_input)\n",
    "print(f\"봇: {bot_response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_OmpkJYN-njL"
   },
   "source": [
    "3. 베스트 오브 설정 전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MKJse_DE-2ym",
    "outputId": "fe27d965-42da-47f8-a394-571471f9bf7a"
   },
   "outputs": [],
   "source": [
    "def generate_code_response(user_input):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"당신은 코드 작성에 도움이 되는 프로그래머입니다.\"},\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "#파이썬으로 정렬 알고리즘을 작성해줘.\n",
    "user_input = input()\n",
    "bot_response = generate_code_response(user_input)\n",
    "print(f\"봇: {bot_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XvY-OBzCCA1a"
   },
   "source": [
    "### OpenAI API를 활용한 코드 생성: 스트리밍 설정의 영향\n",
    "\n",
    "이 노트북에서는 OpenAI API를 사용하여 코드를 생성할 때, 스트리밍 설정이 결과 출력 방식에 미치는 영향을 살펴봅니다.\n",
    "\n",
    "## 1. 기본 설정 (스트리밍 없음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YjU5kQPRBxMK",
    "outputId": "e079350b-c827-4973-a752-e81b4af21bf8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "당신은 코드 작성에 도움이 되는 프로그래머입니다.\n",
      "봇: 물론입니다! 코드 작성에 도움이 필요하시면 어떤 언어나 문제에 대한 것인지 말씀해 주세요. 예제 코드나 특정 질문에 대해 도와드리겠습니다.\n"
     ]
    }
   ],
   "source": [
    "def generate_code_response(user_input):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"당신은 코드 작성에 도움이 되는 프로그래머입니다.\"},\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "#파이썬으로 퀵 정렬 알고리즘을 구현해줘.\n",
    "user_input = input()\n",
    "bot_response = generate_code_response(user_input)\n",
    "print(f\"봇: {bot_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1EULc0yeg2J_"
   },
   "source": [
    "## 2. 스트리밍 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ie6vOa9eCDtI",
    "outputId": "deb183e3-c6e4-40e7-d88f-c86ca63ae8e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "당신은 코드 작성에 도움이 되는 프로그래머입니다.\n",
      "맞습니다! 코드 작성에 도움이 필요하시다면, 어떤 언어와 어떤 문제에 대해 도움을 필요로 하시는지 말씀해 주시면 최선을 다해 도와드리겠습니다. 질문해 주세요!None"
     ]
    }
   ],
   "source": [
    "def generate_streaming_code_response(user_input):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"당신은 코드 작성에 도움이 되는 프로그래머입니다.\"},\n",
    "            {\"role\": \"user\", \"content\": user_input}\n",
    "        ],\n",
    "        stream=True  # 실시간 스트리밍 응답\n",
    "    )\n",
    "    for message in response:\n",
    "        print(message.choices[0].delta.content, end='')\n",
    "#\"파이썬으로 퀵 정렬 알고리즘을 구현해줘. \"\n",
    "user_input = input()\n",
    "generate_streaming_code_response(user_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5EORR6wWg5Tl"
   },
   "source": [
    "1. 응답 출력 방식:\n",
    "   - 기본 설정: 전체 응답이 한 번에 출력됩니다.\n",
    "   - 스트리밍 설정: 응답이 실시간으로 조금씩 출력됩니다.\n",
    "\n",
    "2. 사용자 경험:\n",
    "   - 기본 설정: 사용자는 전체 응답이 준비될 때까지 기다려야 합니다.\n",
    "   - 스트리밍 설정: 사용자는 응답이 생성되는 과정을 실시간으로 볼 수 있습니다.\n",
    "\n",
    "3. 응답 내용:\n",
    "   - 두 설정 모두 퀵 정렬 알고리즘의 파이썬 구현을 제공했습니다.\n",
    "   - 코드의 기본 구조와 설명은 유사하지만, 세부적인 표현에서 약간의 차이가 있습니다.\n",
    "\n",
    "4. 장단점:\n",
    "   - 기본 설정:\n",
    "     - 장점: 전체 응답을 한 번에 처리할 수 있어 후처리가 용이합니다.\n",
    "     - 단점: 긴 응답의 경우 사용자가 오래 기다려야 할 수 있습니다.\n",
    "   - 스트리밍 설정:\n",
    "     - 장점: 사용자에게 즉각적인 피드백을 제공하여 대기 시간이 짧게 느껴집니다.\n",
    "     - 단점: 응답을 부분적으로 받기 때문에 전체 응답을 처리하기 위해서는 추가 로직이 필요할 수 있습니다.\n",
    "\n",
    "5. 적용 scenarios:\n",
    "   - 기본 설정: 짧은 응답이나 전체 응답을 한 번에 처리해야 하는 경우에 적합합니다.\n",
    "   - 스트리밍 설정: 긴 응답이나 실시간 상호작용이 중요한 경우(예: 채팅 인터페이스)에 유용합니다.\n",
    "\n",
    "스트리밍 설정은 API 응답의 출력 방식을 크게 변화시킬 수 있습니다. 이는 사용자 경험을 향상시키고,\n",
    "특히 긴 응답을 생성할 때 유용할 수 있습니다. 그러나 응용 프로그램의 요구사항에 따라 적절한\n",
    "설정을 선택해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3FrUhk8khqTs"
   },
   "source": [
    "## 나만의 ChatGPT 모델 만들어 보기\n",
    "이제 OpenAI API를 사용하여 나만의 ChatGPT 모델을 만들고 대화를 나눌 수 있게 해줍니다.\n",
    "다양한 옵션을 조정하여 모델의 응답을 커스터마이즈할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dbJhOXq_CRyo",
    "outputId": "ed0753fd-4351-4ffe-e3e2-bde080b2489c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "나만의 ChatGPT 모델 설정\n",
      "Temperature (0.0-2.0, 기본 0.7): 0.7\n"
     ]
    }
   ],
   "source": [
    "def chat_with_gpt(messages, temperature=0.7, top_p=1.0,\n",
    "                  frequency_penalty=0, presence_penalty=0,\n",
    "                  max_tokens=None, stream=False):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        frequency_penalty=frequency_penalty,\n",
    "        presence_penalty=presence_penalty,\n",
    "        max_tokens=max_tokens,\n",
    "        stream=stream\n",
    "    )\n",
    "\n",
    "    if stream:\n",
    "        full_response = \"\"\n",
    "        for message in response:\n",
    "            if message.choices[0].delta.content is not None:\n",
    "                print(message.choices[0].delta.content, end='', flush=True)\n",
    "                full_response += message.choices[0].delta.content\n",
    "        print()  # 줄바꿈\n",
    "        return full_response\n",
    "    else:\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "def get_float_input(prompt, default, min_val, max_val):\n",
    "    while True:\n",
    "        try:\n",
    "            value = input(f\"{prompt} ({min_val}-{max_val}, 기본 {default}): \")\n",
    "            if value == \"\":\n",
    "                return default\n",
    "            value = float(value)\n",
    "            if min_val <= value <= max_val:\n",
    "                return value\n",
    "            else:\n",
    "                print(f\"값은 {min_val}에서 {max_val} 사이여야 합니다.\")\n",
    "        except ValueError:\n",
    "            print(\"유효한 숫자를 입력해주세요.\")\n",
    "\n",
    "print(\"나만의 ChatGPT 모델 설정\")\n",
    "temperature = get_float_input(\"Temperature\", 0.7, 0.0, 2.0)\n",
    "top_p = get_float_input(\"Top_p\", 1.0, 0.0, 1.0)\n",
    "frequency_penalty = get_float_input(\"Frequency penalty\", 0, -2.0, 2.0)\n",
    "presence_penalty = get_float_input(\"Presence penalty\", 0, -2.0, 2.0)\n",
    "max_tokens = int(input(\"Max tokens (기본: None): \") or 0) or None\n",
    "stream = input(\"스트리밍을 사용하시겠습니까? (y/n, 기본 n): \").lower() == 'y'\n",
    "\n",
    "print(\"\\n채팅을 시작합니다. 종료하려면 '종료'를 입력하세요.\")\n",
    "messages = [{\"role\": \"system\", \"content\": \"당신은 유용한 AI 어시스턴트입니다.\"}]\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"당신: \")\n",
    "    if user_input.lower() == '종료':\n",
    "        print(\"채팅을 종료합니다.\")\n",
    "        break\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "    ai_response = chat_with_gpt(messages, temperature, top_p,\n",
    "                                frequency_penalty, presence_penalty,\n",
    "                                max_tokens, stream)\n",
    "    messages.append({\"role\": \"assistant\", \"content\": ai_response})\n",
    "\n",
    "    if not stream:\n",
    "        print(f\"AI: {ai_response}\")\n",
    "\n",
    "print(\"\\n채팅 기록:\")\n",
    "for message in messages[1:]:  # 시스템 메시지 제외\n",
    "    print(f\"{message['role'].capitalize()}: {message['content']}\")\n",
    "\n",
    "print(\"\\n설정한 옵션들의 의미:\")\n",
    "print(\"- Temperature:\", temperature, \"- 높을수록 더 창의적이고 다양한 응답을 생성합니다.\")\n",
    "print(\"- Top_p:\", top_p, \"- 낮출수록 더 보수적인 응답을 생성합니다.\")\n",
    "print(\"- Frequency penalty:\", frequency_penalty, \"- 높일수록 반복을 줄이고 다양성을 증가시킵니다.\")\n",
    "print(\"- Presence penalty:\", presence_penalty, \"- 높일수록 새로운 주제를 더 자주 도입합니다.\")\n",
    "print(\"- Max tokens:\", max_tokens, \"- 응답의 최대 길이를 제한합니다.\")\n",
    "print(\"- Stream:\", stream, \"- 실시간으로 응답을 받아볼 수 있습니다.\")\n",
    "\n",
    "print(\"\\n이 설정들을 조정해가며 여러분만의 AI 모델을 만들어보세요!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bIkVcmkbhzU0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
